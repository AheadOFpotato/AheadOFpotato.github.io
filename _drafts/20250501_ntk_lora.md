---
layout: post
title: a review for neural tangent kernel (NTK), lora and pissa
date: 2025-05-01 15:32:00
description: notes for studying neural tangent kernel, and review peft methods for LLM finetuning
tags: LLM-reasoning
categories: sample-posts
---

# Neural Tangent Kernel (NTK)
NTKs are a useful tool to understand the learning dynamics of NN and implicit regularization in gradient descent.

## Definition
NTK 衡量的是：在使用SGD（随机梯度下降）训练时，使用样本 $x'$ 来做一步参数更新后，对样本 $x$ 的预测值（$f(x)$）的影响。这里，我们把训练过程建模成一个连续过程，即每一步的learning rate非常小，记为 $\eta$：
$$
k(x, x') = \lim_{\eta\rightarrow 0}\frac{f(x,
\theta+\eta \frac{d f_\theta (x')}{d \theta})-f(x,\theta)}{\eta}
$$

对上式做泰勒展开：
$$
f(x,
\theta+\eta \frac{d f_\theta (x')}{d \theta}) = f(x, \theta) + \eta \cdot f'(x, \theta) \cdot f'_\theta(x', \theta)\\
\therefore \quad k(x, x') = f'(x, \theta) \cdot f'_\theta(x', \theta) = \langle \frac{df(x,\theta)}{d\theta}, \frac{df(x',\theta)}{d\theta}\rangle
$$

其中求导记号，如 $f', f_\theta '$ 均为对 $\theta$ 求导，即对参数求导。

NTK描述的是，训了某个样本 $x'$，会对另一个样本 $x$ 的预测值产生多大的影响。计算上需要对 $x, x'$ 分别求一次导。

## Properties we like
1. 在无限宽的网络中，如果参数在以某种合适的分布下初始化，那么在该初始值下的 NTK 是一个确定的函数，这意味着，不管我的初始值是多少，最终总会收敛到一个确定的核函数上，它与初始化无关！ 
2. 而且在无限宽网络中，并不会随着训练的变化而变化，也就是说，在训练中参数的改变并不会改变该核函数。

以上两个事实告诉我们，在无限宽网络中，训练可以理解成一个简单的 kernel gradient descent，且kernel还是固定的，只取决于网络的结构还有激活函数之类的。

> cited from "直观理解Neural Tangent Kernel", https://zhuanlan.zhihu.com/p/339971642


## 推荐学习资料
* [YouTube Lecture](https://www.youtube.com/watch?v=DObobAnELkU&t=4s)
* [博客园：Neural Tangent Kernel （NTK）基础推导
](https://www.cnblogs.com/manuscript-of-nomad/p/17243296.html)
* [知乎：直观理解Neural Tangent Kernel](https://zhuanlan.zhihu.com/p/339971642)

# LoRA and PiSSA

LoRA 和 PiSSA 均为一种 parameter-efficient fine-tuning 的方法。在微调阶段，固定原始模型的权重，在原始模型之外训练low-rank adapters。low-rank adapters是对全参数微调的一种低秩近似，虽然丧失一些精度，但能够大大节省显存开销。

![fig](https://pica.zhimg.com/v2-a844adda13c0c4e3a511ac11d3602b60_1440w.jpg)

以上是从PiSSA作者知乎里拿来的图。PiSSA和LoRA的区别只在初始化不同，以下我们主要介绍两种PEFT方法的初始化。

## LoRA的初始化
首先，对每个线性变换（比如attention中的$x\rightarrow Q, K, V$，比如MLP中的两层线性变换）而言，LoRA涉及到3个不同的权重参数：
1. Pretrained Matrix: $W\in\mathcal{R}^{m\times n}$
2. A Matrix: $A\in \mathcal{R}^{m\times r}$
3. B Matrix: $B\in \mathcal{R}^{r\times n}$

其中，$m$为线性层的输入维度，$n$为输出维度，$r$为低秩近似这里的“秩”，是可以额外调节的超参数。我们训练时，只对 $A, B$ 训练，这样训练参数量就由原来的 $m\times n$ 减小为 $m\times r + n\times r$。通常，$m,n$ 是几百量级，$r$ 常取 $4, 8, 16, 32$。

以下为具体的$A, B$矩阵的初始化形式：
```python
nn.init.normal_(self.lora_A.weight, std=1 / self.r)
nn.init.zeros_(self.lora_B.weight) 
```

$A$ 矩阵用 Gaussian 随机初始化，$B$ 矩阵零初始化。

## PiSSA的初始化
```python
# 在peft包中的LoRA初始化方式后面：
nn.init.normal_(self.lora_A.weight, std=1 / self.r)
nn.init.zeros_(self.lora_B.weight) 
# 增加一种PiSSA初始化可选项：
Ur, Sr, Vr = svd_lowrank(self.base_layer.weight, self.r, niter=4) 
# 注意：由于self.base_layer.weight的维度是(out_channel,in_channel,所以AB的顺序相比图示颠倒了一下)
self.lora_A.weight = torch.diag(torch.sqrt(Sr)) @ Vh.t()
self.lora_B.weight = Ur @ torch.diag(torch.sqrt(Sr)) 
self.base_layer.weight = self.base_layer.weight - self.lora_B.weight @ self.lora_A.weight
```

首先，我们对 Pretrained Matrix 做奇异值分解:
$$W = USV^T$$

然后，我们希望对 $W$ 中对应的奇异值最大的部分进行微调，因此，我们取出前 $r$ 个奇异值和它们对应的 $S, V$的部分，放入 $A, B$ 矩阵中。留下 Residual Matrix $= U_{[r:]} S_{[r:]} V^T_{[r:]}$.

$A$ 矩阵初始化: $A = U_{[:r]} \sqrt{S_{[:r]}}$ ，$B$ 矩阵初始化: $B = \sqrt {S_{[:r]}} V^T_{[:r]}$。


# 当 NTK 遇见 PEFT
一个线性层，$W\in\mathcal{R}^{d\times d'}$,输入$X\in\mathcal{R}^{n\times d}$, 输出：$Z=XW\in\mathcal{R}^{n\times d'}$。loss 对 $W$ 的梯度为：
$$
G = \frac{\partial l}{\partial W} = X^T g
$$

其中，$g = \frac{\partial l}{\partial Z}, g\in\mathcal{R}^{n\times d'}$，为上游梯度。

若用LoRA（$W\rightarrow W'+AB$, $A\in \mathcal{R}^{d\times r}, B\in \mathcal{R}^{r\times d'}$），则现在 l 对 weight 的梯度变为：
$$
\frac{\partial l}{\partial B} = A^T G = A^TX^T g
$$
$$
\frac{\partial l}{\partial A} = G B^T = X^T g B^T
$$

则原先的NTK $k(x_1, x_2) = \langle \frac{\partial l(x_1, \theta, W)}{\partial W}, \frac{\partial l(x_2, \theta, W)}{\partial W} \rangle=\langle G_1, G_2\rangle$,会变为:

* 对$B$部分求导：$\langle A^T G_1, A^T G_2\rangle = \langle AA^T G_1, G_2\rangle$
* 对$A$部分求导：$\langle G_1 B^T, G_2 B^T\rangle = \langle G_1 B^T B, G_2\rangle$
* 这里，矩阵内积的定义为element-wise求和加起来，注意：
  * $\langle X, Y \rangle = \sum_{i,j} X_{ij} Y_{ij} = \text{tr}(X^T Y)$

**影响**：
* 对PiSSA而言：
  * 回顾PiSSA：
    * $A$ 矩阵初始化: $A = U_{[:r]} \sqrt{S_{[:r]}}$ 
    * $B$ 矩阵初始化: $B = \sqrt {S_{[:r]}} V^T_{[:r]}$
  * 则初始化时，
  * $A A^T = U_{[:r]} S_{[:r]} U_{[:r]}^T$
  * $B^T B = V_{[:r]} {S_{[:r]}} V^T_{[:r]}$
* 对LoRA而言：
  * $A A^T = 0$
  * $[B^T B]_{ij} = B_{ki} B_{kj} = \sigma^2 \mathbb{I}(i=j)$
  * 在r很大时收敛至1。而当$r$相对小的时候，LoRA下的NTK与原模型会有区别。

# Cross-task LoRA for Improved Transfer

## 思想
* NTK表示的是在一个sample上训练，对另一个sample的预测值的影响。在此我们把预测值理解为loss。
* 在语言模型中，我们需要通过微调使得模型适应某个专项领域。但通常，微调会产生以下问题：语言模型微调后，虽然能在这个专项领域的训练集上取得较好的表现，对同分布的测试集也可以有较好的泛化能力，但对于分布外的情况，泛化能力较差。
* 具体而言，一个场景是，我们对语言模型微调一个长文本，模型可以背出这些文本，但并没有办法在这个长文本上进行推理。又或者是，我们希望通过finetune模型的spelling能力，来增加下游其他character-level的任务的性能，但实际上发现单纯在spelling上finetune并没有办法完成任务之间的迁移。

## 方法

$$
W\rightarrow W' + AB
$$
其中$W'=W-AB$是不训的。

对于$A, B$的初始化，我们进行如下优化：
* $A:\min\quad -\langle A A^T G_1, G_2\rangle - \lambda (\text{tr}(AA^T)-\text{sum}(AA^T - diag(AA^T)))$
* $B:\min\quad -\langle G_1 B^T B, G_2\rangle - \lambda (\text{tr}(BB^T)-\text{sum}(BB^T - diag(BB^T)))$