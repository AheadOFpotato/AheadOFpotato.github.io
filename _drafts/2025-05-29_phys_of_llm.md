---
layout: post
title: Physics of Language Models é˜…è¯»ç¬”è®°
date: 2025-05-29 15:32:00
description: åœ¨ICMLå¬äº†Allen Zhuçš„tutorialï¼Œè§‰å¾—å¾ˆæœ‰å¯å‘ï¼Œæœ€è¿‘å‘ç°phys of LMsç³»åˆ—è¿˜åœ¨ä¸æ–­æ›´æ–°ï¼Œå†™ä¸€ç¯‡é˜…è¯»ç¬”è®°ç³»ç»Ÿæ•´ç†ä¸€ä¸‹ã€‚
tags: LLM-reasoning, mechanism interpretability
categories: sample-posts
---

## Paper List:
1. [Physics of Language Models: Part 1, Learning Hierarchical Language Structures](https://arxiv.org/abs/2305.13673)
2. [Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process](https://openreview.net/forum?id=Tn5B6Udq3E)
3. [Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems](https://arxiv.org/abs/2408.16293)
4. [Physics of Language Models: Part 3.1, Knowledge Storage and Extraction](https://arxiv.org/abs/2309.14316)
5. [Physics of Language Models: Part 3.2, Knowledge Manipulation](https://arxiv.org/abs/2309.14402)
6. [Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://arxiv.org/abs/2404.05405)
7. [Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers.](https://ssrn.com/abstract=5240330)

# Physics of Language Models: Part 1, Learning Hierarchical Language Structures
LLMçš„reasoning mechanismä¸€ç›´æ˜¯ä¸€ä¸ªçƒ­è®®è¯é¢˜ï¼Œå…ˆå‰å·¥ä½œä¸»è¦åšcopy/selectionè¿™æ ·çš„simple symbolic tasksï¼Œæœ¬æ–‡æ¢è®¨ä¸€ä¸ªæ›´éš¾çš„taskï¼šcontext-free grammars (CFGs).

æœ€ç»ˆèƒ½å¤Ÿå¾—åˆ°ä»¥ä¸‹insightsï¼š
* ä¸ºä½•ç»å¯¹ä½ç½®ç¼–ç ï¼ˆAbsolute PEsï¼‰åŠ£äºæ—‹è½¬å½¢å¼çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆrelative and rotary PEsï¼Œè¿™é‡ŒæŒ‡çš„åº”è¯¥å°±æ˜¯RoPEï¼‰
* uniform attentionå¾ˆå¼ºäº†
* encoder-onlyçš„æ¨¡å‹åœ¨CFGçš„deep structure reasoningå±‚é¢åŠ£äºauto-regressiveæ¨¡å‹
* åœ¨ä¸è®­ç»ƒæ•°æ®ä¸­æ³¨å…¥structural or syntactic noiseæ˜¾è‘—æå‡æ¨¡å‹åœ¨corrupted promptsä¸Šçš„é²æ£’æ€§

## Previous Work and Their Claims
1. **A Simple Case**: attention headå¯ä»¥åšæ‹¬å·åŒ¹é…ï¼ˆCan transformers learn to solve problems recursively?ï¼‰
2. **çŸ¥è¯†å­˜å‚¨**ï¼štransformerçš„MLPå±‚å­˜äº†ä¸€äº›key-valueå½¢å¼çš„çŸ¥è¯†ï¼ˆROMEï¼ŒMEMITç­‰å·¥ä½œï¼‰
3. **Induction Heads**ï¼š
   * transformeræœ‰ä¸€äº›å¤´èƒ½å­˜ä¸€äº›æ›´ä¸ºæŠ½è±¡çš„featureï¼Œè€Œä¸åªæ˜¯tokenå±‚é¢çš„matching 
   > They â€œhypothesizedâ€ that induction heads may exist to â€œmatch and copy more abstract and sophisticated linguistic features, rather than precise tokensâ€, yet they acknowledge that they â€œdonâ€™t have a strong framework for mechanistically understandingâ€ this.
4. **ä¸€äº›å…³äºlogical reasoningçš„reverse engineering**ï¼šæå‡ºäº†ä¸åŒåŠŸèƒ½çš„attention heads
   > Most notably, they explained how GPT2 predicts the next token â€œMaryâ€ given prefix â€œWhen Mary and John went to the store, John gave a drink to [...]â€ This requires some logical reasoning by selecting (not naively copying) what is the right name. 

## Motivation
ç›®å‰æ¨¡å‹å·²ç»å¾ˆå¼ºäº†ï¼Œæˆ‘ä»¬å¾€å¾€å®é™…ä¸Šå…³å¿ƒçš„æ˜¯ä¸€äº›éå¸¸éš¾çš„reasoning tasksï¼Œå¯èƒ½èƒŒåæ²¡æœ‰å¾ˆå¥½çš„algotirhmï¼Œä¸åƒä¹‹å‰ç ”ç©¶çš„ç±»ä¼¼copyï¼Œselectionï¼Œsortingè¿™æ ·çš„tasksã€‚é‚£æˆ‘ä»¬èƒ½å¦æå‡ºä¸€ä¸ªsettingå»ç ”ç©¶æ›´ä¸ºå¤æ‚çš„ä»»åŠ¡ï¼Ÿ
* å¸Œæœ›èƒ½å¤Ÿæ•æ‰åˆ°é•¿ç¨‹dependency
* å¸Œæœ›æœ‰ä¸€äº›local ambiguityï¼Œå¸Œæœ›æ‰¾ä¸€äº›CFGæ˜¯éœ€è¦global planning

## Preliminaries
* synthetic CFGs, ä½œè€…åˆ›å»ºäº†ä¸€å¥—æ¯”è¾ƒå¤æ‚çš„CFGè§„åˆ™ã€‚
  * åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦ç¬¦åˆè¿™æ®µè¯­æ³•æ˜¯æ¯”è¾ƒå›°éš¾çš„ï¼Œéœ€è¦ç”¨DPä¹‹ç±»çš„æ€æƒ³ã€‚
  * ä¸”ä¿è¯è¦æœ‰local ambiguousï¼Œä½¿å¾—æ¨¡å‹ä¸å¤ªèƒ½æœ‰shortcut
  * è¯­æ³•æ ‘æ·±åº¦å¯ä»¥è¢«æ‰©å±•å¾—å¾ˆæ·±
   ![fig](./20250529_phys_of_llm/cfg.png) 

## Main Conclusions
### Results 1-3: Transformers can learn such CFGs
* GPTè¿™ç§æ¶æ„å¯ä»¥å­¦ä¼šCFG
* ç”¨rotataryæˆ–relative attentionæ˜¯å¾ˆå¿…è¦çš„ï¼Œå°¤å…¶å¯¹äºæ¯”è¾ƒå¤æ‚çš„CFGè€Œè¨€
* é€šè¿‡attention patternæˆ–è€…hidden statesç»™å‡ºä¸€äº›è§£é‡Š
#### evidence
ä½œè€…åœ¨ä¸Šè¿°CFGä¸Šgenerateäº†ä¸€ä¸ªç¬¦åˆè¯­æ³•è§„èŒƒçš„large corpusï¼Œå¹¶åœ¨ä¸Šé¢pretrainäº†ä¸€ä¸ªdecoder-only transformerã€‚æŠŠæ¯ä¸€ä¸ªtermial tokenä½œä¸ºä¸€ä¸ªseparate tokenã€‚

å®éªŒçš„æ¨¡å‹ä¸ºGPT2-small(12-layer, 12-head, 768-dimensions)ï¼Œåˆ©ç”¨ä»¥ä¸‹PEï¼š
1. $\text{GPT}_{rel}$: ä½¿ç”¨å¦‚ä¸‹çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œåœ¨hidden stateä¸Šconcat**ç›¸å¯¹ä½ç½®ç¼–ç ** for attentionçš„è®¡ç®—
   * ![fig](./20250529_phys_of_llm/rel_pos.png)
2. $\text{GPT}_{rot}$: ç”¨RoPE
3. $\text{GPT}_{pos}$: æŠŠattention matrixç›´æ¥æ›¿æ¢æˆ$A_{i,j}$ä»…ä¾èµ–äº$i,j$ç›¸å¯¹ä½ç½®çš„å½¢å¼ï¼Œä½†è¿™ä¸ª$A_{i,j}=f(i,j)$æ˜¯å¯ä»¥è®­çš„
4. $\text{GPT}_{uni}$ï¼šç”¨fixä½çš„attention matrixï¼Œç¬¬hä¸ªå¤´ç”¨uniform average over the previous $2^h-1$ä¸ªtokenï¼ˆï¼Ÿä»€ä¹ˆæ„æ€ï¼‰

ä»¥ä¸Šæ¨¡å‹ï¼Œé™¤äº†åŸå§‹çš„GPTï¼ˆç”¨çš„ç»å¯¹ä½ç½®ç¼–ç ï¼‰éƒ½å¯ä»¥å­¦åˆ°synthetic CFGï¼Œç»™å‡ºä»»æ„å‰ç¼€ï¼Œå‡å¯ç”Ÿæˆcompletion stringsæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
* **accuracy**: æ»¡è¶³CFG rules
* **diversity**: åœ¨CFG languageä¸­æ»¡è¶³ä¸€å®šçš„å¤šæ ·æ€§
* **probability**: distributionä¸Šæ»¡è¶³CFGç”Ÿæˆæ—¶çš„åˆ†å¸ƒ

<font color=#FF6384>ğŸ’­ï¼šè¿™é‡Œæ¨¡å‹çš„æ³›åŒ–æ€§å¦‚ä½•ç•Œå®šï¼Ÿç‰¹åˆ«æ˜¯èƒ½å¦ç•Œå®šæ¨¡å‹å­¦åˆ°äº†å“ªä¸€ä¸ªhierachyçš„ruleï¼Ÿtest sampleä¸­çš„sequenceæ˜¯å¦æ²¡æœ‰å‡ºç°åœ¨è¿‡pretraining corpusä¸­ï¼Ÿ</font>

å®éªŒç»“æœå¦‚ä¸‹ï¼š
![fig](./20250529_phys_of_llm/result-1-3.png)
1. å·¦å›¾å±•ç¤ºäº†ä¸åŒGPTåœ¨ä¸åŒéš¾åº¦çš„CFGä¸Šçš„test accuracyï¼Œå…¶ä¸­cut0è¡¨ç¤ºprompt sequence lengthä¸º0ï¼Œcut50è¡¨ç¤ºprompt sequence lengthä¸º50ã€‚
2. ä¸­é—´çš„å›¾å±•ç¤ºäº†æ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œä½œè€…è®¤ä¸ºï¼Œç”Ÿæˆå¤šæ ·æ€§è¯´æ˜äº†æ¨¡å‹å¹¶ä¸æ˜¯åœ¨é¢„è®­ç»ƒæ—¶ä»…ä»…è®°ä½äº†CFGçš„ä¸€ä¸ªsubsetã€‚
3. å³å›¾å±•ç¤ºäº†å’Œtrue CFG distributionçš„KL divergenceã€‚

<font color=#FF6384>ğŸ’­ï¼šåŒæ ·åœ°ï¼Œæ˜¯å¦ä¾èµ–memorizationæ„Ÿè§‰ç›®å‰çš„è¯æ®å¹¶ä¸å……åˆ†ã€‚å¯ä»¥åšä¸€äº›å¹²é¢„å®éªŒï¼Œæ¯”å¦‚åœ¨pre-training corpusé‡Œé¢å»æ‰æŸä¸ªpatternï¼Œçœ‹æ¨¡å‹æ˜¯å¦ä»ç„¶èƒ½å­¦ä¼šã€‚</font>

### Results 4-5: How do transformers learn CFGs?
* ç”¨multi-head linear probeè¯´æ˜æ¨¡å‹çš„hidden statesä¸­å­˜äº†NTï¼ˆnonterminalï¼‰çš„ä¿¡æ¯
* è€ŒBERTè¿™æ ·çš„æ¶æ„å­¦ä¸åˆ°

#### evidence
**Multi-head Linear Probe.** åœ¨Transformeræœ€åä¸€å±‚ï¼Œåšmulti-head linear probe,å¯¹ä¸åŒçš„å¤´è®­ä¸åŒçš„linear headï¼Œæœ€åç”¨å¯è®­ç»ƒhead weightåŠ èµ·æ¥ã€‚

linear probeè¯´æ˜GPT modelså¯ä»¥åœ¨æœ€åä¸€å±‚encodeè¿›NT ancestorå’ŒNT boundaryçš„ä¿¡æ¯ï¼š
![fig](./20250529_phys_of_llm/result-4.png)

![fig](./20250529_phys_of_llm/result-5.png)

![fig](./20250529_phys_of_llm/result-5-2.png)

<font color=#FF6384>ğŸ’­ï¼šæœ€åä¸€å±‚probeå‡ºæ¥ï¼Œè¿™å¯¹äºresult 1-3ï¼ˆæ¨¡å‹å¯ä»¥å­¦åˆ°CFGçš„distributionï¼‰ä¹‹å¤–æœ‰æ–°çš„ä¿¡æ¯é‡å—ï¼Ÿæ¢å¥è¯è¯´ï¼Œæœ€åçš„next token predictionå°±æ˜¯æ ¹æ®æœ€åä¸€å±‚hidden stateåšçš„ï¼Œé‡Œé¢ä¸€å®šæœ‰distributionçš„ä¿¡æ¯ï¼Œé‚£æ ¹æ®distributionæ˜¯å¦å¯ä»¥ç›´æ¥é¢„æµ‹ancestorï¼Œè€Œè¿™å¹¶ä¸è¯´æ˜æ¨¡å‹å†…éƒ¨æ˜¯æœ‰ä¸€æ£µCFG treeçš„ã€‚</font>


### Results 6-9
* ç”¨ä¸€äº›æ–¹æ³•å¯è§†åŒ–/é‡åŒ–attention pattern
* è¯´æ˜GPTå¯ä»¥å­¦position-basedå’Œboundary-based attentions
* å¯¹GPTå¦‚ä½•åšè¿™ç§hierarchical
### Result 11
* åªåœ¨æ­£ç¡®æ ·ä¾‹ä¸Šåšè®­ç»ƒçš„GPT robustnessè¾ƒå·®
* åŠ 10%çš„perbutationå¯ä»¥å¤§å¹…æå‡æ¨¡å‹é²æ£’æ€§
* -->åœ¨é¢„è®­ç»ƒé˜¶æ®µåŠ ç¨å¾®å·®ä¸€ç‚¹çš„æ•°æ®å¯èƒ½æœ‰å¥½å¤„
### Result 12-13
* GPTä¼šå‘å±•å‡ºä¸€ç§æ¨¡å¼å¼€å…³ï¼Œåœ¨æ˜¯å¦ä¼šçŠ¯è¯­æ³•é”™è¯¯ä¸Šåšå‡ºè½¬æ¢

