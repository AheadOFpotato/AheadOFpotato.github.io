2025.8.1

先记录一些有趣的topic：

1. Interpreting the Repeated Token Phenomenon in Large Language Models (https://arxiv.org/pdf/2503.08908)
   1. 关于softmax是否需要归一化，为何需要，attention sink本质原因是什么，是否需要改进
   2. softmax is not enough (for sharp size generalization)
2. muon optimizer？ gradient descent does not do type check
3. sparsity matters for length generalization?

---

discrete diffusion相关：

1. The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More;
2. The Pitfalls of Next-Token Prediction;
3. Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions;
4. BEYOND AUTOREGRESSION: DISCRETE DIFFUSION FOR COMPLEX REASONING AND PLANNING;
5. Diffusion Beats Autoregressive in Data-Constrained Settings
6. Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models
7. A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers

---

第一篇paper：

1. continuous diffusion
   * TCS这篇有些seperation results
   * synthetic 框架下分析，continuous/discrete + AR/NAR，哪些任务上哪些组合的setting是work
     * 自己的组合pretrain
     * MDM可以用别人model
     * theoretically continuous diffusion上限更高
   * pretrain 8卡 H100 5-10天 --> pretrain model
   * best paper: analytical

 * continuous + discrete
   * continuous target是embedding
   * 从mask/uniform noise去
 * pretrain 的 codebase 用 MDLLM diffusion duality
   * https://arxiv.org/pdf/2406.07524
   * https://github.com/s-sahoo/duo
   * Generalized Interpolating Discrete Diffusion

2. learn order
   * 区分AR/NAR

3. latent CoT RL
   * RL for compute-dense tasks
   * layer compute
