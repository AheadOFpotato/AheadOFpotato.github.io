---
layout: post
title: Surveying Latent Chain-of-Thoughts + Reinforcement Learning in Large Language Models
date: 2025-07-23 10:32:00
description: ‰∏§‰ª∂ÂØπreasoningÂæàÈáçË¶ÅÁöÑ‰∫ãÔºöscale up computeÔºåÂ•ΩÂ•ΩÂàÜÈÖçcompute
tags: LLM-reasoning, mechanism interpretability
categories: sample-posts
---
- [X] (2025.5) Hybrid Latent Reasoning via Reinforcement Learning
- [X] (2025.5) Reinforced Latent Reasoning for LLM-based Recommendation
- [X] (2025.5) Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space
- [ ] (2025.5) Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains
- [ ] (2025.7) CTRLS: Chain-of-Thought Reasoning via Latent State-Transition
- [X] (2025.5) Continuous Chain of Thought Enables Parallel Exploration and Reasoning
- [ ] (2025.5) Do language models use their depth efficiently?
- [ ] Understanding and minimising outlier features in transformer training

# Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space
*2025.5.19 BIGAI, Zilong*

* **LATENTSEEK** leverages policy gradient to iteratively update latent representations, guided by self-generated reward signal
* Test-Time Instance-Level Adaptation (TTIA) does **not** require parameter updating and operates on a *per-instance basis* during the *testing phase*
![1753328677040](image/2025-07-23_latent_CoT_RL/1753328677040.png)

## Test-Time Instance-Level Adaptation (TTIA)
### problem formulation
auto-regressive language model $\pi$, a reasoning problem instance $c$ as context prompt:
$$
\pi(x|c) = \prod_{t=1}^{T} \pi(x_t | x_{<t}, c)
$$

at test time, the ground truth is unknown, introduce a reward function $R(x, c)$:
$$
x^* = \argmax_x R(x, c)
$$
### TTIA with policy gradient in latent space
for a given input sequence $x$, denote a corresponding sequence of latent representations $z = (z_1, z_2, \cdots, z_T)$, where $z_t\in\mathbb{R}^d$ lies in the latent space of $x_t$, ËøôÈáåËØ¥ÁöÑÊòØLM head‰πãÂâçÁöÑÊúÄÂêé‰∏ÄÂ±Çhidden state„ÄÇ

ÂàôTTIA in the latent spaceÁöÑ‰ºòÂåñÁõÆÊ†áÂ¶Ç‰∏ãÔºö
$$
z^* = \argmax_z \mathbb{E}_{x\sim\pi(x|z)}[R(x, c)]
$$

> üí≠ comment: ËøôÂ§™Â•áÊÄ™‰∫ÜÔºåËøôÊ†∑language modelÂè™ÊòØencoderÁöÑ‰ΩúÁî®ÔºüreasoningÊåáÊúõÂú®ËøôÊúÄÂêé‰∏ÄÂ±ÇlatentÈÄöËøáself modificationÂÆåÊàêÔºü

* sampling strategy
  * ÂÉèÊú¨Êù•ÁöÑÊ®°Âûã‰∏ÄÊ†∑ÔºåÊüê‰∏™‰ΩçÁΩÆÁöÑlatentÂè™ÁÆ°ÈÇ£‰∏™‰ΩçÁΩÆÁöÑtoken
  * $\pi(x|z) = \prod_{t=1}^T \pi(x_t|z_t)$
* optimization procedure
  * $z\leftarrow z+\eta \nabla_z \mathcal{J}(z)$
  * $\nabla_z\mathcal{J}(z) = \mathbb{E}_{x\sim\pi(x|z)}[R(x, c)\nabla_z \log\pi(x|z)]$
* reward function
  * latent space $\rightarrow$ greedy decoding $\rightarrow$ self-reward prompt + pretrained language model $\rightarrow$ compute the reward
  * $R(x, c)\sim \pi(\cdot|\tilde{x}, c, \text{prompt}_{\text{self-reward}})$

ÊÄª‰ΩìÊÄùÊÉ≥ÔºöÁî®LLMÁöÑself-rewardÂΩìreward functionÊù•‰ºòÂåñÊúÄÂêé‰∏ÄÂ±ÇÁöÑhidden stateÔºåÂÉè‰∏Ä‰∏™ËæÉ‰∏∫Âçé‰∏ΩÁöÑself-verification+self-consistency

### performance
Â∫îËØ•‰øùÊåÅBoNÂíåLATENTSEEK iterationÊï∞Áõ∏ÂêåÁöÑÔºö
![1753328714025](image/2025-07-23_latent_CoT_RL/1753328714025.png)


# Hybrid Latent Reasoning via Reinforcement Learning
2025.5.24 UIUC

* introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that 
  * (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism
  * (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features

## Methods
* ÊääÊúÄÂêé‰∏ÄÂ±Çhidden statesÂØπtoken embeddingÂÅöÂä†ÊùÉÂíåtoken embeddingÂä†Ëµ∑Êù•ÔºåÁÑ∂ÂêéÂÅöÊ≠£Â∏∏ÁöÑRL„ÄÇ
* Êúâ‰∏Ä‰∏™gatingÊú∫Âà∂ÂÜ≥ÂÆöhidden stateÂíåsampled token's token embeddingÁöÑÂç†ÊØî
![1753332193670](image/2025-07-23_latent_CoT_RL/1753332193670.png)

## Performance

![1753332925446](image/2025-07-23_latent_CoT_RL/1753332925446.png)

# Reinforced Latent Reasoning for LLM-based Recommendation
* discuss applications in reconmmendation system
* two-stage strategy:
  * first, supervised finetuning to initialize the latent reasoning module
  * second, pure RL training to encourage exploration through a rule-based reward design
* add an attention layer on top of the LLM's final decoding layer $\rightarrow$ extracts information from the final hidden states of the LLM to generate a latent reasoning token aligned with the LLM's input embedding space

> üí≠ ‰∏∫‰ªÄ‰πàÊòØ‰∏Ä‰∏™attentionÂ±ÇÔºü

## Latent reasoning architecture
![1753341552405](image/2025-07-23_latent_CoT_RL/1753341552405.png)
1. introduce an additional attention layer on top of the final decoding layer to explicitly generate latent thought tokens
   1. dedicated reasoning generator that aggregates contextual information?
   2. ensure the generated tokens are aligned with the LLM's input embedding space
2. ÂÆöÈïølatent token

# :star: Continuous Chain of Thought Enables Parallel Exploration and Reasoning
* continuously-valued tokens
(CoT2) 
  * examines the benefits of CoT2 through logical reasoning tasks that inherently require search capabilities 
  * provide optimization and exploration methods for CoT2
* interpretability
  * CoT2 allows the model to track multiple traces in parallel and quantify its benefits for inference efficiency
  * one layer transformer equipped with CoT2 can provably solve the combinatorial "subset sum problem"
* several sampling strategies:
  1. samples and composes $K$ discrete tokens ar each decoding step to control the level of parallelism (reduce to standard CoT when $K=1$)
  2. continuous exploration over the probability simplex
  3. policy optimization with CoT2 indeed inproves the performance of the model beyond its initial discrete and continuous supervision

## Introduction
Motivation:
1. modern language models may not fully utilize their potential for a few reasons:
   1. discrete sampling of tokens limits the model to emitting at most $\log_2(v)$ ($v$ is the size of vocabulary) bits per sample, or more specifically, the Shannon entropy of the softmax output. This contrasts with the $O(d)$ bits each token embedding can store
   2. Secondly, discrete sampling can cause the model to commit to certain solutions and avoid exploring alternatives 

Method: CoT with Continuous Tokens (CoT2), built on COCONUT
* rather than the model sampling a single token from the vocabulary, it samples or deterministically *selects a continuous superposition of tokens according to the softmax output*

Contributions:
* Mechanistic and theoretical study of CoT2
  * theoretical: single-layer transformer can solve Minimum Non-Negative Sum (MNNS) using CoT2
  * theoretical study for:
    * **Base CoT2**: deterministic inference which creates and feeds continuous tokens using full softmax output at each step
    * **CoT2-MTS (multi-token sampling)**: samples $K$ discrete tokens from softmax and averages them to form a continuous token (maybe like top-k version of Base CoT2)
* Supervision and reinforcement for CoT2
    * distill, like distill multiple CoT traces into CoT2 continuous tokens
    * policy optimization

![1753345485106](image/2025-07-23_latent_CoT_RL/1753345485106.png)

## Problem Setup
Objective: given $X\in\mathbb{R}^{n\times d}$, where each of the $n$ rows is a d-dimensional embedding vector, the objective is to output $m$ tokens where the $m^{th}$ token to be the final answer.

Èô§‰∫Üfinal answerÔºåÂâç$m-1$‰∏™tokenÂùá‰∏∫continuousÔºö
For the first $m-1$ steps, the model outputs continuous tokens $\{z_t\}_{t\in [m-1]}$, at the final step $t=m$, the model outputs a discrete token $z_m$ from a vocabulary of size $v$.

ÂØπ‰∫é$1\leq t\leq m-1$, the model outputs the following probability distribution over $v$ vocabulary entries via a softmax operation:
$$
LM_{\theta}(\cdot|z_{<t}, X):=\alpha_t
$$

$$
z_t = E^\top \alpha_t\in\mathbb{R}^d
$$

where $E$ is the embedding matrix $E = [e_1, \cdots, e_v]^\top\in \mathbb{R}^{v\times d}$

### CSFT: A Supervised Training Method for CoT2
supervise at each token, specify a target probability distribution:
$$
\alpha_t^* = [\alpha_{t,1}^*, \cdots, \alpha_{t, v}^*]
$$

train the model to minimize the cross entropy between model output and this distribution

2 ways to provide prefix:
1. use ground truth prefix
2. use previous generated continuous tokens

latent tokenÊï∞ÁõÆÂõ∫ÂÆö
### Reinforcement Learning Methods for CoT2
![1753348504269](image/2025-07-23_latent_CoT_RL/1753348504269.png)

# Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains
![1754275568218](image/2025-07-23_latent_CoT_RL/1754275568218.png)

## overview
* 2-stage training approach
  * SFT: next-token prediction by incorporating an auxiliary next compressed embedding prediction objective
  * RL
* perform reasoning at a dense latent level; dynamically adjust reasoning speed

## motivation regarding previous attempts
1. regarding efficiency work about long-CoT
   1. enhancing efficiency at the token level, primarily by identifying and skipping less informative tokens
   2. prompting models to generate more concise reasoning steps
   3. dynamically terminating reasoning when the model exhibits high confidence in a trial answer
2. regarding latent CoT
   1. Initial efforts attempt to ‚Äúinternalize‚Äù reasoning knowledge by curriculum learning or knowledge distillation
   2. Some works focus on the potential inside LLMs by looping or skipping some intermediate LLM layers
   3. Recent innovations have introduced auto-regressive prediction of latent representations for efficient reasoning
```
limitations:
1. fixed-length reasoning chains
2. use deterministic latent reasoning processes
```

## method
![1754279550004](image/2025-07-23_latent_CoT_RL/1754279550004.png)

* utilize an auxiliary next compressed embedding prediction task in SFT stage
  * at each training step, CoLaR first samples a random compression factor $c\in[1, c_{max}]$ and merges the embeddings of $c$

1. Reasoning token compression and understanding
   1. Ê†πÊçÆcompression rateÔºåbegin each training step by randomly sampling $c\in[1, r_{max}]$, for every $r$ consecutive reasoning token embeddings $e_{r}^{k:k+r}$, the embedding compress module generates a compressed embedding $e_c^k$
   2. embedding compress module: 
      1. apply mean pooling: 
         * Simply averaging these embeddings can distort the original distribution. 
         * For instance, consider two uncorrelated distributions $A\sim N(\mu, \sigma^2)$, $B\sim N(\mu, \sigma^2)$
         * mean pooling would alter the original distribution to $\frac{A+B}{2}$, effectively scaling the variance by $\frac{1}{2}$. We found that, for most pre-trained LLMs, the distributions of embeddings are centered at $\mu\approx 0$. Thus, to prevent distortion of the original embedding distribution of LLMs, the Embedding Compress module only scales the sum of the $r$ embeddings by $\frac{1}{\sqrt{r}}$
   3. ÊÄé‰πàÁªôloss
      1. Âè™ÈÄöËøáanswerÊù•Áªô‰∏ç‰Ω≥ÔºåÁõëÁù£Â§™Á®ÄÁñè‰∫ÜÊ≤°Ê≥ïÊî∂Êïõ
      2. train‰∏Ä‰∏™modelÊù•predict compressed reasoning token, for each compressed token embedding, CoLaR should be able to **read** and **predict** tokens in groups of *r*.
    $$
    \mathcal{L}_{\text{comp}} = -\frac{1}{L_a+L_c}\sum\limits_{i=1}^{L_a+L_c} \log p([\textbf{t}_c, \textbf{t}_a]^i)
    $$
2. Next compressed embedding prediction
   1. explore in the latent space $\rightarrow$ given the current hidden states $h_c^i$ output by $\mathcal{M}$, the latent head $\mathcal{E}$ predicts both the mean $\mu_c^{i+1}$ and the standard deviation $\sigma_c^{i+1}$ of the next embedding's distribution $\rightarrow$ $e_c^{i+1}=\mu_c^{i+1}+\sigma_c^{i+1}\epsilon$
   2. the latent head is primarily trained using the negative log-likelihood loss, for a prediction at position $i$, this can be formulated as:
   $$
    \mathcal{L}_{\text{latent}}(i) = -\log p(e_c^i|\mu_c^i, \sigma_c^i) = \frac{(e_c^i-\mu_c^i)^2}{2\sigma_c^i}+\log \sigma_c^i
   $$

   3. we empirically found that CoLaR with NLL loss tends to under-fit on simpler math reasoning datasets that require less exploratio $\rightarrow$ propose a soft-MSE loss that combines Mean Squared Error with an entropy regularization term
   $$
    \mathcal{L}_{\text{latent}}(i) = \mathbb{E}_\epsilon [(\mu_c^i+\sigma_c^i \epsilon -e_c^i)^2] - \alpha \left(\frac{1}{2}\log (2\pi e (\sigma_c^i)^2)\right)
   $$

3. Exploration with reinforcement learning
   1. use GRPO

## Performance
* use Llama-3.2-1B
![1754297672240](image/2025-07-23_latent_CoT_RL/1754297672240.png)
![1754297772431](image/2025-07-23_latent_CoT_RL/1754297772431.png)

<!-- # CTRLS: Chain-of-Thought Reasoning via Latent State-Transition -->

# Do language models use their depth efficiently?

## Motivation
Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers?

* analyze the residual stream of Llama-3.1 & Qwen 3 family of models
* results:
  1. compare the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, **with a clear phase transition**
  2. skip layers in the second half has a much smaller effect on future computations and output predictions
  3. for multi-hop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops
  4. we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation
     1.  train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the **larger model simply spreads the same computations out over its many layers**.

All this evidence suggests that deeper models are **not** using their depth to learn new kinds of computation, but only **using the greater depth to perform more fine-grained adjustments to the residual**.

## more fine-grained motivation
* ‰∏ÄÊñπÈù¢Ôºå‰πãÂâçÂæàÂ§öÂ∑•‰ΩúÂèëÁé∞ÔºåÂØπlayerÂÅöpruning‰∏ç‰ºöÊçü‰º§Ê®°ÂûãÊÄßËÉΩÔºõÂÆûÈ™å‰∏äÔºåd Gromov et al. were able to remove half of the layers from the network without significantly affecting performance on MMLU (but not for math)
* Âè¶‰∏ÄÊñπÈù¢ÔºåÂèØËß£ÈáäÊÄßÁ†îÁ©∂ÂèëÁé∞Ê®°Âûã‰∏çÂêåÂ±ÇÁ°ÆÂÆûÂú®ÂÅö‰∏çÂêåÁöÑÊìç‰Ωú

Êú¨ÊñáÁöÑÊ†∏ÂøÉÈóÆÈ¢òÔºö**do deeper LLMs
use their depth to compose more features to create higher-order computations that are impossible
in shallow models, or do they merely spread the same kinds of computation out over more layers?**

![1754301754707](image/2025-07-23_latent_CoT_RL/1754301754707.png)

## Methods
* Ê®°Âûã‰∏ªË¶ÅÁî®ÔºöLlama-3.1-70B
* ÂèØËß£ÈáäÊÄßrepoÔºöNDIF and NNsight
### 1. How do the Layers Interact With the Residual Stream?
Âõ†‰∏∫ÊâÄÊúâTransformer‰∏≠Ë∑üresidual streamÁöÑ‰∫íÂä®ÈÉΩÊòØadditiveÁöÑÔºåÊâÄ‰ª•ÂèØÈ¢ÑÊúüÁöÑÊòØÔºå$||h_l||_2$‰ºöÈöèlayerÂ¢ûÈïøËÄåÂ¢ûÈïø„ÄÇ
* residual growthÂú®outlier featureÂíåuniversal transformer‰∏≠ÈÉΩÊúâË¢´ËßÇÂØüÂà∞Ëøá„ÄÇ

Âú®ÂàùÂßãÊó∂Ôºå$||a_l||_2$Âíå$||m_l||_2$Âú®ÊúüÊúõÊÑè‰πâ‰∏ãÊó∂‰∏ÄÊ†∑ÁöÑÔºåas inputÈÉΩÂÅö‰∫Ünormalization„ÄÇÂõ†Ê≠§Ôºålater layerÁõ∏ÊØîearlier layersÂØπresidualÁöÑË¥°ÁåÆÂáèÂ∞ëÔºöÊîπÂèòresidualÊñπÂêëÊõ¥Èöæ„ÄÇ

><font color=yellow>‰∏∫‰ªÄ‰πàlater layerË¥°ÁåÆÊõ¥Â∞ëÔºü</font>

> Âõ†‰∏∫normÈÄêÂ±ÇÂèòÂ§ßÔºåËÄå$||a_l||_2$Âíå$||m_l||_2$ÊúüÊúõ‰∏çÂèòÔºåÂàôÊòØ‰∏Ä‰∏™ËæÉÂ∞èÁöÑÊï∞Âä†Âà∞‰∏Ä‰∏™ËæÉÂ§ßÁöÑÊï∞‰∏äÂéªÔºåÁÑ∂ÂêéÂÜçÂÅönormÔºåÊïÖÂΩ±ÂìçÈÄêÂ±ÇÂèòÂ∞è

ËÆ≠ÁªÉÊó∂ÔºåÊ®°ÂûãÂèØ‰ª•ÈÄöËøáÂú®later layerÂ¢ûÂä†weight normÊù•Ë°•ÂÅøËøôÁßçresidualË¥°ÁåÆÂáèÂ∞ëÁöÑË∂ãÂäø„ÄÇÁÑ∂ËÄåÔºåËÆ≠ÁªÉÊó∂Âä†‰∫Üweight decayÔºåÊâÄ‰ª•ËøôÁßçcompensateÂèà‰ºöË¢´ÊäëÂà∂„ÄÇ

Ê≠§Â§ÑÔºå‰ΩúËÄÖÊÉ≥ÈÄöËøáresidual growthÁöÑËßíÂ∫¶Êù•ÁúãÊØè‰∏ÄÂ±ÇÁöÑË¥°ÁåÆÂ∫¶„ÄÇ

1. the relative contribution of sub layers.
   * $||h_l||_2, ||a_l||_2$Âíå$||m_l||_2$ ÈöèÂ±ÇÁöÑÂèòÂåñÂ¶Ç‰∏ãÂõæÔºö
     * ![1754312651834](image/2025-07-23_latent_CoT_RL/1754312651834.png)
   * relative contribution of each (sub)layer ($\frac{||a_l+m_l||_2}{||h_l||}$, $\frac{||a_l||_2}{||h_l||_2}$ and $\frac{||m_l||_2}{||h_l+a_l||_2}$)
     * ÂØπÂêéÈù¢Â±ÇÁöÑÂΩ±ÂìçÂú®‰∏≠Èó¥layerÁöÑÊó∂ÂÄôÁ™ÅÁÑ∂‰∏ãÈôçÔºålast few layersÂΩ±ÂìçÂèà‰∏äÂçá
     * ![1754313353776](image/2025-07-23_latent_CoT_RL/1754313353776.png)
2. the cosine similarity between the residual and sublayer contributions
   1. $\text{cossim}(m_l+a_l,h_l)$
   2. Where features are orthogonal to each other, zero cosine similarity corresponds to writing a new feature to the residual, **negative values correspond to erasing features**, and **positive values mean strengthening an existing feature**.
   3. ÂêéÈù¢ÁöÑÂ±ÇÂíåresidual‰øùÊåÅÁõ∏‰ººÊñπÂêëÔºå

### 2. How do the Layers Influence Downstream Computations?

* Causal intervention for measuring the layers‚Äô importance for the downstream computation
  * first, we run a prompt through the model and log the residual $h_l$
  * second, we run the same prompt again, but this time, we skip layer $s$m by setting $\overline{h}_{s+1}:={h}_{s}$, and we log the intervened model model $\overline{h}_l$
  * third, we measure the relative change in the contribution of layer $l>s:\frac{||(h_{l+1}-h_l)-(\overline{h}_{l+1}-\overline{h}_l)||}{||h_{l+1}-h_l||_2}$
  * ![1754312627474](image/2025-07-23_latent_CoT_RL/1754312627474.png)
* Measuring layer importance for future predictions
  * measure the effect on the future tokens when skipping layers for earlier tokens. We do this by sampling a position $1 < t_s < T ‚àí 1$, skipping the layer only for token positions $t \leq t_s$, and measuring the effect only on positions $t > t_s$.
* apply logitlens to the middle-layer hidden states $\rightarrow$ 
  * the prediction refinement seems to start at the same position at the same phase transition as when the layers do not influence the future predictions anymore, where the cosine similarities change sign, and the layer‚Äôs importance decreases
  * ![1754313691846](image/2025-07-23_latent_CoT_RL/1754313691846.png)

### 3. Do Deeper Problems Use Deeper Computation?

We expect that:
1. transformers use more layers to do deeper computation;
2.  later steps of a composite computation should be executed in later layers, so that they can receive the results of earlier subproblems as inputs

but do models really orgnanize their computations this way?

* **Residual erasure interventions**:
  1. integrated gradients: compute the gradient on all answer tokens, but not on the prompt
  2. maximum prediction norm change: 

* **The Depth Score**:
  1. 